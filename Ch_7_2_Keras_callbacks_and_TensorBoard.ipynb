{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch 7-2: Keras callbacks and TensorBoard.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOnIzJmfB6bLtMFoRd2cGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uditmanav17/DL-with-Python/blob/master/Ch_7_2_Keras_callbacks_and_TensorBoard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-VddEYByPEQ",
        "colab_type": "text"
      },
      "source": [
        "##7.2 Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\n",
        "In this section, we’ll review ways to gain greater access to and control over what goes on inside your model during training. Launching a training run on a large dataset for tens of epochs using model.fit() or model.fit_generator() can be a bit like launching a paper airplane: past the initial impulse, you don’t have any control over its trajectory or its landing spot. If you want to avoid bad outcomes (and thus wasted paper airplanes), it’s smarter to use not a paper plane, but a drone that can sense its environment, send data back to its operator, and automatically make steering decisions based on its current state. The techniques we present here will transform the call to model.fit() from a paper airplane into a smart, autonomous drone that can selfintrospect and dynamically take action. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Y1tkiG-MdV",
        "colab_type": "text"
      },
      "source": [
        "###7.2.1 Using callbacks to act on a model during training\n",
        "When you’re training a model, there are many things you can’t predict from the start. In particular, you can’t tell how many epochs will be needed to get to an optimal validation loss. The examples so far have adopted the strategy of training for enough epochs that you begin overfitting, using the first run to figure out the proper number of epochs to train for, and then finally launching a new training run from scratch using this optimal number. Of course, this approach is wasteful.\n",
        "\n",
        "A much better way to handle this is to stop training when you measure that the validation loss in no longer improving. This can be achieved using a Keras callback. A callback is an object (a class instance implementing specific methods) that is passed to the model in the call to fit and that is called by the model at various points during training. It has access to all the available data about the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model.\n",
        "\n",
        "Here are some examples of ways you can use callbacks:\n",
        "* Model checkpointing—Saving the current weights of the model at different points during training.\n",
        "* Early stopping—Interrupting training when the validation loss is no longer improving (and of course, saving the best model obtained during training).\n",
        "* Dynamically adjusting the value of certain parameters during training—Such as the learning rate of the optimizer.\n",
        "* Logging training and validation metrics during training, or visualizing the representations learned by the model as they’re updated—The Keras progress bar that you’re familiar with is a callback!\n",
        "\n",
        "The keras.callbacks module includes a number of built-in callbacks (this is not an exhaustive list):\n",
        "```\n",
        "keras.callbacks.ModelCheckpoint\n",
        "keras.callbacks.EarlyStopping\n",
        "keras.callbacks.LearningRateScheduler\n",
        "keras.callbacks.ReduceLROnPlateau\n",
        "keras.callbacks.CSVLogger\n",
        "```\n",
        "Let’s review a few of them to give you an idea of how to use them: ModelCheckpoint, EarlyStopping, and ReduceLROnPlateau.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhlLytQ1-Q2m",
        "colab_type": "text"
      },
      "source": [
        "#### THE MODELCHECKPOINT AND EARLYSTOPPING CALLBACKS\n",
        "You can use the EarlyStopping callback to interrupt training once a target metric being monitored has stopped improving for a fixed number of epochs. For instance, this callback allows you to interrupt training as soon as you start overfitting, thus avoiding having to retrain your model for a smaller number of epochs. This callback is typically used in combination with ModelCheckpoint, which lets you continually save the model during training (and, optionally, save only the current best model so far: the version of the model that achieved the best performance at the end of an epoch):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wgbd6aVyKza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "# Callbacks are passed to the model via the\n",
        "# callbacks argument in fit, which takes a list of\n",
        "# callbacks. You can pass any number of callbacks.\n",
        "callbacks_list = [\n",
        "                  # Interrupts training when improvement stops\n",
        "                  keras.callbacks.EarlyStopping( \n",
        "                      # Monitors the model’s validation accuracy\n",
        "                      monitor='acc', \n",
        "                      # Interrupts training when accuracy has stopped improving \n",
        "                      # for more than one epoch (that is, two epochs)\n",
        "                      patience=1,\n",
        "                      ),\n",
        "                  # Saves the current weights after every epoch\n",
        "                  keras.callbacks.ModelCheckpoint(\n",
        "                      # Path to the destination model file\n",
        "                      filepath='my_model.h5',\n",
        "                      # These two arguments mean you won’t overwrite the\n",
        "                      # model file unless val_loss has improved, which allows\n",
        "                      # you to keep the best model seen during training.\n",
        "                      monitor='val_loss',\n",
        "                      save_best_only=True,\n",
        "                      )\n",
        "                  ]\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              # You monitor accuracy, so it should\n",
        "              # be part of the model’s metrics.\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Note that because the callback will monitor validation loss and\n",
        "# validation accuracy, you need to pass validation_data to the call to fit.\n",
        "model.fit(x, y,\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tTtcXLZ9_JP",
        "colab_type": "text"
      },
      "source": [
        "####THE REDUCELRONPLATEAU CALLBACK\n",
        "You can use this callback to reduce the learning rate when the validation loss has stopped improving. Reducing or increasing the learning rate in case of a loss plateau is is an effective strategy to get out of local minima during training. The following example uses the ReduceLROnPlateau callback: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKKwJfg2-hYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks_list = [\n",
        "                  keras.callbacks.ReduceLROnPlateau(\n",
        "                      # Monitors the model’s validation loss\n",
        "                      monitor='val_loss',\n",
        "                      # Divides the learning rate by 10 when triggered\n",
        "                      factor=0.1,\n",
        "                      # The callback is triggered after the validation\n",
        "                      # loss has stopped improving for 10 epochs\n",
        "                      patience=10,\n",
        "                      )\n",
        "                  ]\n",
        "# Because the callback will monitor the validation loss, you\n",
        "# need to pass validation_data to the call to fit.                  \n",
        "model.fit(x, y,\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DGwvqRhAFTf",
        "colab_type": "text"
      },
      "source": [
        "#### WRITING YOUR OWN CALLBACK\n",
        "If you need to take a specific action during training that isn’t covered by one of the built-in callbacks, you can write your own callback. Callbacks are implemented by subclassing the class keras.callbacks.Callback. You can then implement any number of the following transparently named methods, which are called at various points during training:\n",
        "\n",
        "* on_epoch_begin - Called at the start of every epoch\n",
        "* on_epoch_end - Called at the end of every epoch\n",
        "\n",
        "* on_batch_begin - Called right before processing each batch\n",
        "* on_batch_end - Called right after processing each batch\n",
        "\n",
        "* on_train_begin - Called at the start of training\n",
        "* on_train_end - Called at the end of training\n",
        "\n",
        "These methods all are called with a logs argument, which is a dictionary containing information about the previous batch, epoch, or training run: training and validation metrics, and so on. Additionally, the callback has access to the following attributes:\n",
        "* self.model—The model instance from which the callback is being called\n",
        "* self.validation_data—The value of what was passed to fit as validation data\n",
        "\n",
        "Here’s a simple example of a custom callback that saves to disk (as Numpy arrays) the activations of every layer of the model at the end of every epoch, computed on the first sample of the validation set: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9zo2uyTBNr8",
        "colab_type": "code",
        "outputId": "267efec7-a2ba-49ca-823c-4da7b507b1ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "class ActivationLogger(keras.callbacks.Callback):\n",
        "    def set_model(self, model):\n",
        "        # Called by the parent model before training, to inform\n",
        "        # the callback of what model will be calling it\n",
        "        self.model = model\n",
        "        layer_outputs = [layer.output for layer in model.layers]\n",
        "        # Model instance that returns the activations of every layer\n",
        "        self.activations_model = keras.models.Model(model.input, layer_outputs)\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.validation_data is None:\n",
        "            raise RuntimeError('Requires validation_data.')\n",
        "        # Obtains the first input sample of the validation data\n",
        "        validation_sample = self.validation_data[0][0:1]\n",
        "        activations = self.activations_model.predict(validation_sample)\n",
        "        with open('activations_at_epoch_' + str(epoch) + '.npz', 'w') as f:\n",
        "            np.savez(f, activations)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcZQQu6kKf4n",
        "colab_type": "text"
      },
      "source": [
        "This is all you need to know about callbacks—the rest is technical details, which you can easily look up. Now you’re equipped to perform any sort of logging or preprogrammed intervention on a Keras model during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRCAqsUtMu5s",
        "colab_type": "text"
      },
      "source": [
        "###7.2.2 Introduction to TensorBoard: the TensorFlow visualization framework\n",
        "To do good research or develop good models, you need rich, frequent feedback about what’s going on inside your models during your experiments. That’s the point of running experiments: to get information about how well a model performs—as much information as possible. Making progress is an iterative process, or loop: you start with an idea and express it as an experiment, attempting to validate or invalidate your idea. You run this experiment and process the information it generates. This inspires your next idea. The more iterations of this loop you’re able to run, the more refined and powerful your ideas become. Keras helps you go from idea to experiment in the least possible time, and fast GPUs can help you get from experiment to result as quickly as possible. But what about processing the experiment results? That’s where TensorBoard comes in.\n",
        "\n",
        "This section introduces TensorBoard, a browser-based visualization tool that comes packaged with TensorFlow. Note that it’s only available for Keras models when you’re using Keras with the TensorFlow backend.\n",
        "\n",
        "The key purpose of TensorBoard is to help you visually monitor everything that goes on inside your model during training. If you’re monitoring more information than just the model’s final loss, you can develop a clearer vision of what the model does and doesn’t do, and you can make progress more quickly. TensorBoard gives you access to several neat features, all in your browser:\n",
        "* Visually monitoring metrics during training\n",
        "* Visualizing your model architecture\n",
        "* Visualizing histograms of activations and gradients\n",
        "* Exploring embeddings in 3D\n",
        "Let’s demonstrate these features on a simple example. You’ll train a 1D convnet on the IMDB sentiment-analysis task. You’ll consider only the top 2,000 words in the IMDB vocabulary, to make visualizing word embeddings more tractable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXZVGEeUKhty",
        "colab_type": "code",
        "outputId": "695e0355-3143-4b30-9893-94615e14c06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "# Text-classification model to use with TensorBoard\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 2000\n",
        "max_len = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.Embedding(max_features, 128,input_length=max_len,name='embed'))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embed (Embedding)            (None, 500, 128)          256000    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 291,937\n",
            "Trainable params: 291,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz_6OrqTeZfi",
        "colab_type": "text"
      },
      "source": [
        "Before you start using TensorBoard, you need to create a directory where you’ll store the log files it generates.\n",
        "```\n",
        "$ mkdir my_log_dir\n",
        "```\n",
        "Let’s launch the training with a TensorBoard callback instance. This callback will write log events to disk at the specified location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rp8MWNjeZBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir my_log_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5wgJ1S5ew9H",
        "colab_type": "code",
        "outputId": "9127ee29-0f15-46f0-854a-931e43fd5653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        }
      },
      "source": [
        "# Training the model with a TensorBoard callback\n",
        "callbacks = [\n",
        "             keras.callbacks.TensorBoard(\n",
        "                 # Log files will be written at this location\n",
        "                 log_dir='my_log_dir',\n",
        "                 # Records activation histograms every 1 epoch\n",
        "                 histogram_freq=1,\n",
        "                 # Records embedding data every 1 epoch\n",
        "                 embeddings_freq=1,\n",
        "                 )\n",
        "             ]\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v2.py:102: UserWarning: The TensorBoard callback does not support embeddings display when using TensorFlow 2.0. Embeddings-related arguments are ignored.\n",
            "  warnings.warn('The TensorBoard callback does not support '\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 2/20\n",
            "20000/20000 [==============================] - 67s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 3/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 4/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 5/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 6/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 7/20\n",
            "20000/20000 [==============================] - 67s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 8/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 9/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 10/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 11/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 12/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 13/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 14/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 15/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 16/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 17/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 18/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 19/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
            "Epoch 20/20\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgikyRgOewbQ",
        "colab_type": "text"
      },
      "source": [
        "At this point, you can launch the TensorBoard server from the command line, instructing it to read the logs the callback is currently writing. The tensorboard utility should have been automatically installed on your machine the moment you installed TensorFlow (for example, via pip):\n",
        "```\n",
        "$ tensorboard --logdir=my_log_dir\n",
        "```\n",
        "You can then browse to http://localhost:6006 and look at your model training. In addition to live graphs of the training and validation metrics, you get access to the Histograms tab, where you can find pretty visualizations of histograms of activation values taken by your layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBS6klIp5F4c",
        "colab_type": "text"
      },
      "source": [
        "#### Tensorboard for Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH87R8JLgypt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7JR6WMxH5ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir .\\my_log_dir"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}