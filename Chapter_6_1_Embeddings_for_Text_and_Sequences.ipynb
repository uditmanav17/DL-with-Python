{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 6-1: Embeddings for Text and Sequences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1LmGRzLmBU0ACnXN_V_6uhNESuaTYQ1Zz",
      "authorship_tag": "ABX9TyOOLpqLMTzX6TB5neSbxB2G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uditmanav17/DL-with-Python/blob/master/Chapter_6_1_Embeddings_for_Text_and_Sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDVNWuDjUPf",
        "colab_type": "text"
      },
      "source": [
        "# DL for Text and Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfPvug8-jOxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBGWLREgjgTO",
        "colab_type": "text"
      },
      "source": [
        "This chapter explores deep-learning models that can process text (understood as\n",
        "sequences of word or sequences of characters), timeseries, and sequence data in\n",
        "general. The two fundamental deep-learning algorithms for sequence processing\n",
        "are recurrent neural networks and 1D convnets, the one-dimensional version of the 2D convnets that we covered in the previous chapters. We’ll discuss both of these approaches in this chapter.\n",
        "\n",
        "Applications of these algorithms include the following:\n",
        "* Document classification and timeseries classification, such as identifying the\n",
        "topic of an article or the author of a book\n",
        "* Timeseries comparisons, such as estimating how closely related two documents or two stock tickers are\n",
        "* Sequence-to-sequence learning, such as decoding an English sentence into\n",
        "French\n",
        "* Sentiment analysis, such as classifying the sentiment of tweets or movie reviews\n",
        "as positive or negative\n",
        "* Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data\n",
        "\n",
        "This chapter’s examples focus on two narrow tasks: sentiment analysis on the IMDB dataset, a task we approached earlier in the book, and temperature forecasting. But the techniques demonstrated for these two tasks are relevant to all the applications just listed, and many more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBHoasM5mGix",
        "colab_type": "text"
      },
      "source": [
        "## 6.1 Working with text data\n",
        "The deep-learning sequence-processing models introduced in the following sections can use text to produce a basic form of natural-language understanding, sufficient for applications including document classification, sentiment analysis, author identification, and even question-answering (QA) (in a constrained context). Of course, keep in mind throughout this chapter that none of these deeplearning models truly understand text in a human sense; rather, these models can map the statistical structure of written language, which is sufficient to solve many simple textual tasks. Deep learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels.\n",
        "\n",
        "Like all other neural networks, deep-learning models don’t take as input raw text: they only work with numeric tensors. Vectorizing text is the process of transforming text into numeric tensors. This can be done in multiple ways:\n",
        "* Segment text into words, and transform each word into a vector.\n",
        "* Segment text into characters, and transform each character into a vector.\n",
        "* Extract n-grams of words or characters, and transform each n-gram into a vector. N-grams are overlapping groups of multiple consecutive words or characters.\n",
        "\n",
        "Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens, and breaking text into such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens. These vectors, packed into sequence tensors, are fed into deep neural networks. There are multiple ways to associate a vector with a token. In this section, I’ll present two major ones: one-hot encoding of tokens, and token embedding (typically used exclusively for words, and called\n",
        "word embedding). The remainder of this section explains these techniques and shows how to use them to go from raw text to a Numpy tensor that you can send to a Keras network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enVJiqU9s0gE",
        "colab_type": "text"
      },
      "source": [
        "## Understanding n-grams and bag-of-words\n",
        "Word n-grams are groups of N (or fewer) consecutive words that you can extract from a sentence. The same concept may also be applied to characters instead of words.\n",
        "Here’s a simple example. Consider the sentence “The cat sat on the mat.” It may be decomposed into the following set of 2-grams:<br>\n",
        "{\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}<br>\n",
        "It may also be decomposed into the following set of 3-grams:<br>\n",
        "{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}<br>\n",
        "Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag here refers to the fact that you’re dealing with a set of tokens rather than a list or sequence: the tokens have no specific order. This family of tokenization methods is called bag-of-words.<br>\n",
        "Because bag-of-words isn’t an order-preserving tokenization method (the tokens generated are understood as a set, not a sequence, and the general structure of the sentences is lost), it tends to be used in shallow language-processing models rather than in deep-learning models. Extracting n-grams is a form of feature engineering, and deep learning does away with this kind of rigid, brittle approach, replacing it with hierarchical feature learning. One-dimensional convnets and recurrent neural networks, introduced later in this chapter, are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups,\n",
        "by looking at continuous word or character sequences. For this reason, we won’t\n",
        "cover n-grams any further in this book. But do keep in mind that they’re a powerful, unavoidable feature-engineering tool when using lightweight, shallow text-processing models such as logistic regression and random forests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aiy6g2DdtagV",
        "colab_type": "text"
      },
      "source": [
        "### 6.1.1 One-hot encoding of words and characters\n",
        "One-hot encoding is the most common, most basic way to turn a token into a vector. It consists of associating a unique integer index with every word\n",
        "and then turning this integer index i into a binary vector of size N (the size of the vocabulary); the vector is all zeros except for the i th entry, which is 1. One-hot encoding can be done at the character level, as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuJHpShqjiNs",
        "colab_type": "code",
        "outputId": "82682b72-ea00-4416-81fa-f310537ae7dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Word-level one-hot encoding (toy example)\n",
        "import numpy as np\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "\tfor word in sample.split():\n",
        "\t\tif word not in token_index:\n",
        "\t\t\ttoken_index[word] = len(token_index) + 1\n",
        "max_length = 10\n",
        "results = np.zeros(shape=(len(samples),max_length,max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "\tfor j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "\t\tindex = token_index.get(word)\n",
        "\t\tresults[i, j, index] = 1.\n",
        "print(results, results.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]] (2, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IWUCUFfjiL4",
        "colab_type": "code",
        "outputId": "ca229ea1-b300-4b15-d483-f75d474d8827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "# Character-level one-hot encoding (toy example)\n",
        "import string\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "\tfor j, character in enumerate(sample):\n",
        "\t\tindex = token_index.get(character)\n",
        "\t\tresults[i, j, index] = 1.\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GWoksCTT4HG",
        "colab_type": "text"
      },
      "source": [
        "Note that Keras has built-in utilities for doing one-hot encoding of text at the word level or character level, starting from raw text data. You should use these utilities, because they take care of a number of important features such as stripping special characters from strings and only taking into account the N most common words in your dataset (a common restriction, to avoid dealing with very large input vector spaces)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zBjqaKTjiI_",
        "colab_type": "code",
        "outputId": "447fc4cf-43f7-445f-cb40-add6c4389549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Using Keras for word-level one-hot encoding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M4EObgWUSg_",
        "colab_type": "text"
      },
      "source": [
        "A variant of one-hot encoding is the so-called one-hot hashing trick, which you can use when the number of unique tokens in your vocabulary is too large to handle explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these indices in a dictionary, you can hash words into vectors of fixed size. This is typically done with a very lightweight hashing function. The main advantage of this method is that it does away with maintaining an explicit word index, which saves memory and allows online encoding of the data (you can generate token vectors right away, before\n",
        "you’ve seen all of the available data). The one drawback of this approach is that it’s susceptible to hash collisions: two different words may end up with the same hash, and subsequently any machine-learning model looking at these hashes won’t be able to tell the difference between these words. The likelihood of hash collisions decreases when the dimensionality of the hashing space is much larger than the total number of unique tokens being hashed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc2vkSTijiFf",
        "colab_type": "code",
        "outputId": "c50d11ca-f0cb-4e37-9470-c04bd1be0de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "# Word-level one-hot encoding with hashing trick (toy example)\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        results[i, j, index] = 1.\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z2QoPlbW0hb",
        "colab_type": "text"
      },
      "source": [
        "### 6.1.2 Using word embeddings\n",
        "Another popular and powerful way to associate a vector with a word is the use of dense ***word vectors***, also called ***word embeddings***. Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are lowdimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors); see figure 6.2. Unlike the word vectors obtained via one-hot encoding, word embeddings are learned from data. It’s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large\n",
        "vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information into far fewer dimensions.\n",
        "\n",
        "There are two ways to obtain word embeddings:\n",
        "* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the\n",
        "weights of a neural network.\n",
        "* Load into your model word embeddings that were precomputed using a different machine-learning task than the one you’re trying to solve. These are called\n",
        "pretrained word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L_HXxejXrP4",
        "colab_type": "text"
      },
      "source": [
        "#### LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER\n",
        "The simplest way to associate a dense vector with a word is to choose the vector at random. The problem with this approach is that the resulting embedding space has no structure: for instance, the words accurate and exact may end up with completely different embeddings, even though they’re interchangeable in most sentences. It’s difficult for a deep neural network to make sense of such a noisy, unstructured embedding space.\n",
        "\n",
        "To get a bit more abstract, the geometric relationships between word vectors\n",
        "should reflect the semantic relationships between these words. Word embeddings are meant to map human language into a geometric space. For instance, in a reasonable embedding space, you would expect synonyms to be embedded into similar word vectors; and in general, you would expect the geometric distance (such as L2 distance) between any two word vectors to relate to the semantic distance between the associated words (words meaning different things are embedded at points far away from each other, whereas related words are closer). In addition to distance, you may want specific directions in the embedding space to be meaningful. \n",
        "\n",
        "In real-world word-embedding spaces, common examples of meaningful geometric transformations are “gender” vectors and “plural” vectors. For instance, by adding a “female” vector to the vector “king,” we obtain the vector “queen.” By adding a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature thousands of such interpretable and potentially useful vectors.\n",
        "\n",
        "Is there some ideal word-embedding space that would perfectly map human language and could be used for any natural-language-processing task? Possibly, but we have yet to compute anything of the sort. Also, there is no such a thing as human language—there are many different languages, and they aren’t isomorphic, because a language is the reflection of a specific culture and a specific context. But more pragmatically, what makes a good word-embedding space depends heavily on your task: the perfect word-embedding space for an English-language movie-review sentimentanalysis model may look different from the perfect embedding space for an Englishlanguage legal-document-classification model, because the importance of certain semantic relationships varies from task to task.\n",
        "\n",
        "It’s thus reasonable to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about learning the weights of a layer: the Embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS36C7PkjiCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiating an Embedding layer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000, 64)\n",
        "# The Embedding layer takes at least two arguments: the number of possible tokens\n",
        "# (here, 1,000: 1 + maximum word index) and the dimensionality of the embeddings\n",
        "# (here, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ozyD6193mOL",
        "colab_type": "text"
      },
      "source": [
        "The Embedding layer is best understood as a dictionary that maps integer indices\n",
        "(which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. It’s effectively a dictionary lookup.\n",
        "\n",
        "When you instantiate an Embedding layer, its weights (its internal dictionary of\n",
        "token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model.\n",
        "\n",
        "Let’s apply this idea to the IMDB movie-review sentiment-prediction task that\n",
        "you’re already familiar with. First, you’ll quickly prepare the data. You’ll restrict the movie reviews to the top 10,000 most common words (as you did the first time you worked with this dataset) and cut off the reviews after only 20 words. The network will learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KreOq-qIjhXA",
        "colab_type": "code",
        "outputId": "e259d763-7106-47ea-e626-33f2cea0c265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Loading IMDB Dataset for use with Embedding Layer\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "max_features = 10000  # No. of words to consider as feature\n",
        "maxlen = 20  # Cutoff text after this number of words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
        "    num_words=max_features)  # loads dataset as list of integers\n",
        "    \n",
        "# turns integer list to 2D integer Tensor of shape (samples, maxlen)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q64QlD-QOi9-",
        "colab_type": "code",
        "outputId": "f02a8448-8655-4bd3-95a8-70263c983b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 20)\n",
            "(25000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhT6GtwwNSCa",
        "colab_type": "code",
        "outputId": "b7709a79-2520-4e35-9826-414073c59a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Specifies the maximum input length to the Embedding layer so you can \n",
        "# later flatten the embedded inputs. After the Embedding layer,\n",
        "# the activations have shape (samples, maxlen, 8)\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "# flatten 3D tensor to 2D tensorof shape (samples, maxlen*8)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcxbj8WQPj2b",
        "colab_type": "code",
        "outputId": "882cd91e-b4ee-4e9b-ccf3-df39a79d054b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6747 - acc: 0.6128 - val_loss: 0.6358 - val_acc: 0.6896\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5579 - acc: 0.7524 - val_loss: 0.5371 - val_acc: 0.7286\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4659 - acc: 0.7911 - val_loss: 0.5061 - val_acc: 0.7438\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4192 - acc: 0.8124 - val_loss: 0.4978 - val_acc: 0.7506\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3880 - acc: 0.8306 - val_loss: 0.4957 - val_acc: 0.7538\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3617 - acc: 0.8442 - val_loss: 0.5000 - val_acc: 0.7558\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3391 - acc: 0.8564 - val_loss: 0.5050 - val_acc: 0.7566\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3179 - acc: 0.8666 - val_loss: 0.5142 - val_acc: 0.7542\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.2991 - acc: 0.8767 - val_loss: 0.5244 - val_acc: 0.7514\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.2819 - acc: 0.8852 - val_loss: 0.5344 - val_acc: 0.7496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbYucevhRMuS",
        "colab_type": "text"
      },
      "source": [
        "You get to a validation accuracy of ~76%, which is pretty good considering that you’re only looking at the first 20 words in every review. But note that merely flattening the embedded sequences and training a single Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and sentence structure (for example, this model would likely treat both “this movie is a bomb” and “this movie is the bomb” as being negative reviews). It’s much better to add recurrent layers or 1D convolutional layers on top of the embedded sequences to learn features that take into account each sequence as a whole. That’s what we’ll focus on in the next few sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgugnh5VTbx7",
        "colab_type": "text"
      },
      "source": [
        "#### USING PRETRAINED WORD EMBEDDINGS\n",
        "Sometimes, you have so little training data available that you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary. What do you do then?\n",
        "\n",
        "Instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties—that captures generic aspects of language structure. The rationale behind using pretrained word embeddings in natural-language processing is much the same as for using pretrained convnets in image classification: you don’t have enough data available to learn truly powerful features on your own, but you expect the features that you need to be fairly generic—that is, common visual features or semantic features. In this case, it makes sense to reuse features learned on a different problem.\n",
        "\n",
        "Such word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, lowdimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the [Word2vec algorithm](https://code.google.com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec dimensions capture specific semantic properties, such as gender.\n",
        "\n",
        "Let’s look at how you can get started using GloVe embeddings in a Keras model. The same method is valid for Word2vec embeddings or any other word-embedding database. You’ll also use this example to refresh the text-tokenization techniques introduced a few paragraphs ago: you’ll start from raw text and work your way up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sEeCjU_WrZJ",
        "colab_type": "text"
      },
      "source": [
        "### 6.1.3 Putting it all together: from raw text to word embeddings\n",
        "You’ll use a model similar to the one we just went over: embedding sentences in sequences of vectors, flattening them, and training a Dense layer on top. But you’ll do so using pretrained word embeddings; and instead of using the pretokenized IMDB data packaged in Keras, you’ll start from scratch by downloading the original text data.\n",
        "\n",
        "####DOWNLOADING THE IMDB DATA AS RAW TEXT\n",
        "First, head to http://mng.bz/0tIo and download the raw IMDB dataset. Uncompress it. Now, let’s collect the individual training reviews into a list of strings, one string per review. You’ll also collect the review labels (positive/negative) into a labels list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqwv7pYz-F4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXTRACTING DATASET FROM zip\n",
        "import zipfile\n",
        "file_path = r\"/content/drive/My Drive/Colab Notebooks/Francois Chollet - DL with Python/Datasets/aclImdb.zip\"\n",
        "dest_path = r\"/content/drive/My Drive/Colab Notebooks/Francois Chollet - DL with Python/Datasets/aclImdb\"\n",
        "zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "zip_ref.extractall(dest_path)\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARC8jruuQPf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "imdb_dir = '/content/drive/My Drive/Colab Notebooks/Francois Chollet - DL with Python/Datasets/aclImdb/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXPYvFUUGUJU",
        "colab_type": "code",
        "outputId": "477294b7-b43e-4c64-96e8-2696d93ba0e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print(labels[:5])\n",
        "print(texts[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0]\n",
            "[\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\", \"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\", \"This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\", 'Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i\\'m no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying.', 'When I was little my parents took me along to the theater to see Interiors. It was one of many movies I watched with my parents, but this was the only one we walked out of. Since then I had never seen Interiors until just recently, and I could have lived out the rest of my life without it. What a pretentious, ponderous, and painfully boring piece of 70\\'s wine and cheese tripe. Woody Allen is one of my favorite directors but Interiors is by far the worst piece of crap of his career. In the unmistakable style of Ingmar Berman, Allen gives us a dark, angular, muted, insight in to the lives of a family wrought by the psychological damage caused by divorce, estrangement, career, love, non-love, halitosis, whatever. The film, intentionally, has no comic relief, no music, and is drenched in shadowy pathos. This film style can be best defined as expressionist in nature, using an improvisational method of dialogue to illicit a \"more pronounced depth of meaning and truth\". But Woody Allen is no Ingmar Bergman. The film is painfully slow and dull. But beyond that, I simply had no connection with or sympathy for any of the characters. Instead I felt only contempt for this parade of shuffling, whining, nicotine stained, martyrs in a perpetual quest for identity. Amid a backdrop of cosmopolitan affluence and baked Brie intelligentsia the story looms like a fart in the room. Everyone speaks in affected platitudes and elevated language between cigarettes. Everyone is \"lost\" and \"struggling\", desperate to find direction or understanding or whatever and it just goes on and on to the point where you just want to slap all of them. It\\'s never about resolution, it\\'s only about interminable introspective babble. It is nothing more than a psychological drama taken to an extreme beyond the audience\\'s ability to connect. Woody Allen chose to make characters so immersed in themselves we feel left out. And for that reason I found this movie painfully self indulgent and spiritually draining. I see what he was going for but his insistence on promoting his message through Prozac prose and distorted film techniques jettisons it past the point of relevance. I highly recommend this one if you\\'re feeling a little too happy and need something to remind you of death. Otherwise, let\\'s just pretend this film never happened.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxnaSRegWDt",
        "colab_type": "text"
      },
      "source": [
        "####TOKENIZING THE DATA\n",
        "Let’s vectorize the text and prepare a training and validation split, using the concepts introduced earlier in this section. Because pretrained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), we’ll add the following twist: restricting the training data to the first 200 samples. So you’ll learn to classify movie reviews after looking at just 200 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eUYWAD9gbkx",
        "colab_type": "code",
        "outputId": "10de42f7-b1cb-4db8-8591-e9b6cf044932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # cuts off reviews after 100 words\n",
        "training_samples = 200  # train on 200 samples only\n",
        "validation_samples = 10000  #   Validate on 10000 samples\n",
        "max_words = 10000  # consider only 10000 words\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Splits the data into a training set and a validation set, but first shuffles \n",
        "# the data, because you’re starting with data in which samples are ordered \n",
        "# (all negative first, then all positive)\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 88582 unique tokens.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxUDU0Jaho_P",
        "colab_type": "text"
      },
      "source": [
        "####DOWNLOADING THE GLOVE WORD EMBEDDINGS\n",
        "Go to https://nlp.stanford.edu/projects/glove, and download the precomputed embeddings from 2014 English Wikipedia. It’s an 822 MB zip file called glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or nonword tokens). Unzip it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVoHtsvVhs3g",
        "colab_type": "text"
      },
      "source": [
        "####PREPROCESSING THE EMBEDDINGS\n",
        "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HETSKObHQwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXTRACTING DATASET FROM zip\n",
        "import zipfile\n",
        "file_path = r\"/content/drive/My Drive/Colab Notebooks/Francois Chollet - DL with Python/Trained Models/glove.6B.zip\"\n",
        "dest_path = r\"/content/drive/My Drive/Colab Notebooks/Francois Chollet - DL with Python/Trained Models/glove6B\"\n",
        "zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "zip_ref.extractall(dest_path)\n",
        "zip_ref.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtkmR-RShuIM",
        "colab_type": "code",
        "outputId": "e00081e0-4085-4fb3-fc58-68f4e2bab9a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "glove_dir = dest_path\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No9ep1WhxnRn",
        "colab_type": "text"
      },
      "source": [
        "Next, you’ll build an embedding matrix that you can load into an Embedding layer. It must be a matrix of shape (max_words, embedding_dim), where each entry i contains the embedding_dim-dimensional vector for the word of index i in the reference word index (built during tokenization). Note that index 0 isn’t supposed to stand for any word or token—it’s a placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swccckGQxn9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the GloVe word-embeddings matrix\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u02LANWp01Zc",
        "colab_type": "text"
      },
      "source": [
        "#### DEFINING A MODEL\n",
        "Same architecture as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDLZbnst0-8D",
        "colab_type": "code",
        "outputId": "5421a51d-4ba9-4579-9e1e-8d4baed96a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                320032    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SonZFEhc1GNz",
        "colab_type": "text"
      },
      "source": [
        "#### LOADING THE GLOVE EMBEDDINGS IN THE MODEL\n",
        "The Embedding layer has a single weight matrix: a 2D float matrix where each entry i is the word vector meant to be associated with index i. Simple enough. Load the GloVe matrix you prepared into the Embedding layer, the first layer in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V4ZLNr81Mqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V72B0aYF1Ry1",
        "colab_type": "text"
      },
      "source": [
        "Additionally, you’ll freeze the Embedding layer (set its trainable attribute to False), following the same rationale you’re already familiar with in the context of pretrained convnet features: when parts of a model are pretrained (like your Embedding layer) and parts are randomly initialized (like your classifier), the pretrained parts shouldn’t be updated during training, to avoid forgetting what they already know. The large gradient updates triggered by the randomly initialized layers would be disruptive to the already-learned features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgZRAGzM4rLC",
        "colab_type": "text"
      },
      "source": [
        "#### Training and Evaluating model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjJeC7lM1nli",
        "colab_type": "code",
        "outputId": "1dbc4d50-b831-4222-9e5d-9e460d57b4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "\n",
        "validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 116ms/step - loss: 1.6688 - acc: 0.5350 - val_loss: 0.7219 - val_acc: 0.5001\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 1s 93ms/step - loss: 0.6386 - acc: 0.6350 - val_loss: 0.9514 - val_acc: 0.5003\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 0.5696 - acc: 0.6700 - val_loss: 0.7042 - val_acc: 0.5489\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 96ms/step - loss: 0.5159 - acc: 0.7200 - val_loss: 0.6806 - val_acc: 0.5709\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 104ms/step - loss: 0.2317 - acc: 0.9700 - val_loss: 1.8488 - val_acc: 0.5000\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 1s 95ms/step - loss: 0.3428 - acc: 0.8200 - val_loss: 0.7078 - val_acc: 0.5751\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 1s 97ms/step - loss: 0.0881 - acc: 1.0000 - val_loss: 0.7542 - val_acc: 0.5692\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 0.0534 - acc: 1.0000 - val_loss: 0.7746 - val_acc: 0.5654\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 98ms/step - loss: 0.0403 - acc: 1.0000 - val_loss: 1.3111 - val_acc: 0.5078\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 100ms/step - loss: 0.5677 - acc: 0.7850 - val_loss: 0.7541 - val_acc: 0.5866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw4SPGVv46jl",
        "colab_type": "code",
        "outputId": "e4283b3f-17ea-4b11-87f2-68afc9b1ab6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "# Plotting the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7G8TIRK46EK",
        "colab_type": "text"
      },
      "source": [
        "The model quickly starts overfitting, which is unsurprising given the small number of training samples. Validation accuracy has high variance for the same reason, but it seems to reach the high 50s.\n",
        "\n",
        "Note that your mileage may vary: because you have so few training samples, performance is heavily dependent on exactly which 200 samples you choose—and you’re choosing them at random. If this works poorly for you, try choosing a different random set of 200 samples, for the sake of the exercise (in real life, you don’t get to choose your training data).\n",
        "\n",
        "You can also train the same model without loading the pretrained word embeddings and without freezing the embedding layer. In that case, you’ll learn a taskspecific embedding of the input tokens, which is generally more powerful than pretrained word embeddings when lots of data is available. But in this case, you have only 200 training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sykvGzaF52QL",
        "colab_type": "code",
        "outputId": "c6878e12-9923-4aca-94ab-62c0cba184e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# Training the same model without pretrained word embeddings\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                320032    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSLc3_VQ62Ti",
        "colab_type": "code",
        "outputId": "5ca89bd3-9f07-47ca-e00a-ebec7cd3e66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 200 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.6904 - acc: 0.5650 - val_loss: 0.6937 - val_acc: 0.5088\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.4795 - acc: 0.9850 - val_loss: 0.6980 - val_acc: 0.5150\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.2591 - acc: 0.9950 - val_loss: 0.7050 - val_acc: 0.5164\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.1109 - acc: 1.0000 - val_loss: 0.7316 - val_acc: 0.5123\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0542 - acc: 1.0000 - val_loss: 0.7174 - val_acc: 0.5232\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.7357 - val_acc: 0.5209\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.7445 - val_acc: 0.5190\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.7391 - val_acc: 0.5243\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.7443 - val_acc: 0.5257\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.7327 - val_acc: 0.5315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-YNbg8r64zs",
        "colab_type": "text"
      },
      "source": [
        "Validation accuracy stalls in the low 50s. So in this case, pretrained word embeddings outperform jointly learned embeddings. If you increase the number of training samples, this will quickly stop being the case—try it as an exercise. Finally, let’s evaluate the model on the test data. First, you need to tokenize the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZlW4baM7QYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing the data of the test set\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMpv2wgR7keA",
        "colab_type": "code",
        "outputId": "e9325fd1-9e13-4bdf-a3fc-5de135aa33d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Evaluationg the first model on test data\n",
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 42us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7669090269088745, 0.5788400173187256]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaYiJiGT7qs5",
        "colab_type": "text"
      },
      "source": [
        "You get an appalling test accuracy of 56%. Working with just a handful of training samples is difficult!"
      ]
    }
  ]
}